# -*- coding: utf-8 -*-
"""ft_gemma_2_2b_math_to_nl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oKI0tYaBGUtKLBo3CElKqk0VYUQaDng7
"""

# !pip install unsloth
# import torch
# if torch.cuda.get_device_capability()[0] >= 8:
#     !pip install --no-deps packaging ninja einops "flash-attn>=2.6.3"

from google.colab import drive

drive.mount("/content/drive")

from unsloth import FastLanguageModel
import torch
import copy

# load model từ hf với độ dài ngữ cảnh là 2048, và nén lại ở 4 bit
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/gemma-2-2b",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

temp = copy.deepcopy(model)

# chèn các matrix vào các layer
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

# load dataset
from datasets import load_dataset


dataset = load_dataset(
    "json",
    data_files="/content/drive/MyDrive/train_math_text_flexible.jsonl",
    split="train",
)


print(dataset[0])

# format các mẫu dữ liệu trước khi huấn luyện
prompt = """
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}
"""

EOS_TOKEN = tokenizer.eos_token


def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]

    texts = []
    for instruction, input_text, output_text in zip(instructions, inputs, outputs):
        text = prompt.format(instruction, input_text, output_text) + EOS_TOKEN
        texts.append(text)

    return {"text": texts}


dataset = dataset.map(formatting_prompts_func, batched=True)


train_test = dataset.train_test_split(test_size=0.1, seed=42)

train_ds = train_test["train"]
eval_ds = train_test["test"]

print(dataset[10]["text"])

# test trước khi chừa fine-tune
FastLanguageModel.for_inference(temp)
inputs = tokenizer(
    [
        prompt.format(
            "Viết lại công thức toán học sau bằng ngôn ngữ tự nhiên.",
            "(a + b)(a - b) = a^2 - b^2",
            "",
        )
    ],
    return_tensors="pt",
).to("cuda")

from transformers import TextStreamer

text_streamer = TextStreamer(tokenizer)
_ = temp.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=128,
    eos_token_id=tokenizer.eos_token_id
)

from trl import SFTTrainer
from transformers import TrainingArguments, EarlyStoppingCallback
from unsloth import is_bfloat16_supported

early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3,
    early_stopping_threshold=0.0,
)

args = TrainingArguments(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    num_train_epochs=1,
    learning_rate=2e-4,
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    fp16_full_eval=True,
    per_device_eval_batch_size=2,
    eval_accumulation_steps=4,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=3407,
    logging_steps=10,
    report_to="none",
    output_dir="training_checkpoints",
    save_strategy="steps",
    save_steps=10,
    save_total_limit=3,
    eval_strategy="steps",
    eval_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    dataset_text_field="text",
    max_seq_length=2048,
    dataset_num_proc=2,
    packing=False,
    args=args,
    callbacks=[early_stopping_callback],
)

trainer_stats = trainer.train()

# test lại sau khi huấn luyện
FastLanguageModel.for_inference(model)
inputs = tokenizer(
    [
        prompt.format(
            "Viết lại công thức toán học sau bằng ngôn ngữ tự nhiên.",
            "(a + b)(a - b) = a^2 - b^2",
            "",
        )
    ],
    return_tensors="pt",
).to("cuda")

from transformers import TextStreamer

text_streamer = TextStreamer(tokenizer)
_ = model.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=512,
    eos_token_id=tokenizer.eos_token_id
)

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="/content/training_checkpoints/checkpoint-110",
    max_seq_length=2028,
    dtype=None,
    load_in_4bit=True,
)

# !curl -fsSL https://ollama.com/install.sh | sh

# model.save_pretrained_gguf("equation_text", tokenizer, quantization_method = "f16")

# # tự động merge và đẩy lên hf
# model.push_to_hub_merged(
#     tokenizer=tokenizer,
#     repo_id="trongdung143/equation_text",
#     token="",

# )
